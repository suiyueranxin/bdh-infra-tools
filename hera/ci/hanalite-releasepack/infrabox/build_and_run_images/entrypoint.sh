#!/bin/bash
set -ex

set_env_for_vsystem_test() {
# set Envs for vsystem test
# DOCKER_REGISTRY
# VORA_SYSTEM_PASSWORD VORA_SYSTEM_TENANT_PASSWORD
# for GKE: # KEY_FILE # PROJECT_ID
# for DHAAS-AWS:  # AWS_ACCESS_KEY_ID   #AWS_SECRET_ACCESS_KEY
    DOCKER_SOCK_OPTION=""
    if [[ "${COMPONENT}" == "VSYSTEM" ]]; then
        if [[ "${PROVISION_PLATFORM}" == "GKE" ]]; then
              export KEY_FILE=/infrabox/inputs/${PARENT_INSTALL_JOB}/credential.json
              export PROJECT_ID="sap-p-and-i-big-data-vora"
              export DOCKER_REGISTRY="eu.gcr.io/sap-p-and-i-big-data-vora/${GCP_DOCKER_REGISTRY_SUFFIX}"
        fi
        if [[ "${PROVISION_PLATFORM}" == "DHAAS-AWS" ]]; then
            if [[ -n ${DIS_VERSION} ]]; then
                export DOCKER_REGISTRY="250180771110.dkr.ecr.eu-central-1.amazonaws.com"
            else
                # AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY should be set via the job enviroments
                export DOCKER_REGISTRY="726853116465.dkr.ecr.eu-central-1.amazonaws.com/dev"
            fi
        fi
        export VORA_SYSTEM_PASSWORD=${VORA_SYSTEM_TENANT_PASSWORD}
        DOCKER_SOCK_OPTION=" -v /var/run/docker.sock:/var/run/docker.sock "
    fi
}

get_pr_label() {
    set +e
    label=''
    if [[ -n "${GITHUB_REPOSITORY_FULL_NAME}" ]] && [[ -n "${GITHUB_PULL_REQUEST_NUMBER}" ]]; then
        pr_url=https://github.wdf.sap.corp/api/v3/repos/${GITHUB_REPOSITORY_FULL_NAME}/pulls/${GITHUB_PULL_REQUEST_NUMBER}
        VELOBOT_USER='velobot'
        set +x
        $(curl -XGET ${pr_url} -k -H 'content-type: application/json' -u "${VELOBOT_USER}:${VELOBOT_TOKEN}" -o label.json)
        set -x
        label_list=$(jq '.labels' label.json)
        if [[ "${label_list}" != "[]" ]]; then
            label_str=$(jq '.labels[].name' label.json)
            label_str1=$(echo ${label_str} | sed 's/ /-/g')
            label=$(echo ${label_str1} | sed 's/\"//g')
        fi
     fi
     echo ${label}
}

get_pr_commit() {
    set +e
    commit_messages=''
    if [[ -n "${GITHUB_REPOSITORY_FULL_NAME}" ]] && [[ -n "${GITHUB_PULL_REQUEST_NUMBER}" ]]; then
        pr_url=https://github.wdf.sap.corp/api/v3/repos/${GITHUB_REPOSITORY_FULL_NAME}/pulls/${GITHUB_PULL_REQUEST_NUMBER}/commits
        VELOBOT_USER='velobot'
        set +x
        commits_str=$(curl -XGET ${pr_url} -k -H 'content-type: application/json' -u "${VELOBOT_USER}:${VELOBOT_TOKEN}" | jq '.[].commit.message')
        set -x
        if [[ -n "$commits_str" ]]; then
            # commit_str is like "Update DI 2108.24.0 and federation 2108.6.0\nupdate connection-service(2108.6.0)\nupdate data-tools-ui(2108.13.0)\nupdate datahub-app-base(2108.14.0)\nupdate datahub-flowagent(2108.6.0)\nupdate diagnostics(2108.9.0)\nupdate hana(2108.8.0)\nupdate installer-base(2108.4.0)\nupdate rms(2108.7.0)\nupdate security-operator(2108.9.0)\nupdate storagegateway(2108.16.0)\nupdate uaa(2108.10.0)\nupdate vsolution(2108.4.0)\nupdate vsolution-shared-ui(2108.5.0)\nupdate vsolution-vsystem-ui(2108.5.0)\nupdate vsystem(2108.15.0)\nupdate federation-service(2108.6.0)\n\nðŸ© This change was generated by donut(UTC 2021-08-07 20:18:30)."
            # split commit_str to get each component version
            for commit in ${commits_str[@]}
            do
                if [[ "${commit}" =~ ")\n" ]]; then
                    # the component version item is like 'connection-service(2108.6.0)\nupdate'
                    # remove the substr from '\n'
                    message=${commit%%\\n*}
                    commit_messages=${commit_messages}${message}'-'
                fi
            done
        fi
        # remove the last '-'
        commit_messages=${commit_messages%-*}
    fi
echo ${commit_messages}
}

upload_result_cumulus() {

    if [[ -n "${PIPELINE_ID_CCM}" ]] || [[ -n "${PIPELINE_ID_FEDERATION}" ]] || [[ -n "${PIPELINE_ID_VSYSTEM}" ]]; then
        echo "start upload the result to cumulus"
    else
        echo "skip upload the result to cumulus"
        return 0
    fi

    if [[ "${COMPONENT}" == "CONNECTION_SERVICE" ]]; then
        special_pipelie_id=${PIPELINE_ID_CCM}
    elif [[ "${COMPONENT}" == "FEDERATION_SERVICE" ]]; then
        special_pipelie_id=${PIPELINE_ID_FEDERATION}
    elif [[ "${COMPONENT}" == "VSYSTEM" ]]; then
        special_pipelie_id=${PIPELINE_ID_VSYSTEM}
    fi

    set +x
    echo ${CUMULUS_STORAGE_KEY} > /cumulus_key.json
    set -x
    cumulus_path=$(get_pr_label)
    if [[ "${cumulus_path}" == "" ]]; then
        cumulus_path=$(get_pr_commit)
    fi
    gcloud auth activate-service-account --key-file=/cumulus_key.json
    filenames=$(ls /infrabox/upload/testresult/*.xml)
    for file in ${filenames};do
        echo ${file}
        gsutil cp ${file} gs://${special_pipelie_id}/${cumulus_path}/junit/
    done
    echo "## Result uploaded to sirius completed."
}

set_branch_with_tag(){
  if [[ -n ${COMPONENT} ]]; then
    #Assemble the components version string. Exp: APP_BASE_VERSION="2.3.100" COMPONENT="APP_BASE" COMPONENT_VERSION=APP_BASE_VERSION=2.3.100
    COMPONENT_VERSION=`eval echo '$'"$COMPONENT""_VERSION"`
    if [ -z "${GIT_BRANCH}" ]; then
        # set the tag to env GIT_BRANCH
        GIT_BRANCH=$(git tag | grep /$COMPONENT_VERSION |grep "rel")
    fi
  else
    echo "COMPONENT enviroment not set ."
  fi
}

policy_assign() {
  set +e
  POLICY_FILE="memberAddPolicy.json"
  cat >> ${POLICY_FILE} << EOF
{
    "id": "memberAddPolicy",
    "description": "",
    "enabled": true,
    "exposed": true,
    "resources": null,
    "policyReferences": [
     {
      "id": "sap.dh.modelerStart"
     }
    ]
}
EOF

  Policy_tag=$(vctl policy list-policies | awk '{print $1}' |grep 'memberAddPolicy')
  if [[ -z "${Policy_tag}" ]]; then
    vctl policy create ${POLICY_FILE}
  fi
  vctl policy assign "memberAddPolicy" $1
  sleep 30
  set -e    
}

# create tenant user
create_unique_user() {
  if [ $# -ne 4 ]; then
    echo "## create tenant user: The number of input parameters is wrong"
    exit
  fi
  tenant=$1
  tenant_user=$2
  tenant_password=$3
  tenant_role=$4
  RETRY=3
  for ((i = 1; i <= ${RETRY}; i++)); do
    vctl user create $tenant $tenant_user $tenant_password "$tenant_role"
    if [[ "$tenant_role" == "member" ]]; then
      policy_assign $tenant_user
    fi

    vctl login ${VSYSTEM_ENDPOINT} $tenant $tenant_user -p "$tenant_password" --insecure
    CURRENT_USER=$(vctl whoami | awk -F '[ :]' '{print $4}')
    if [[ "$CURRENT_USER" == "$VORA_USERNAME" ]]; then
      break
    elif [ $i -eq ${RETRY} ]; then
      echo "### [level=error] The attempt to create a unique username has failed."
    else
      sleep 2
    fi
  done
}

version_ge(){
  test "$(echo "$@" | tr " " "\n" | sort -rV | head -n 1)" == "$1"; }

update_pipeline_flag(){
  # DM01-2802 from vsolution 2210.6.0, no need to start pipeline-modeler
  # set the flag to false, if vsolution version >= 2210.6.0
  if [[ "${PROVISION_PLATFORM}" == "DHAAS-AWS" ]] && [[ -n "${VSOLUTION_VERSION}" ]]; then
    if version_ge "${VSOLUTION_VERSION}" "2210.6.0"; then
      pipeline_flag=false
    fi
  fi
}

start_scheduler(){
  for templateId in $TEMPLATEIDS; do
    if [[ "$templateId" == "ml-deployment-api" ]]; then
        continue
    fi
    # DM01-2802 from vsolution 2210.6.0, no need to start pipeline-modeler
    if [[ "${pipeline_flag}" == "false" ]] && [[ "$templateId" == "pipeline-modeler" ]]; then
      continue
    fi
    for ((i = 1; i <= ${RETRY}; i++)); do
      if vctl scheduler start $templateId; then
        break
      elif [ $i -eq ${RETRY} ]; then
        echo "### [level=error] The attempt to start $templateId has failed."
      fi
    done
  done

  if [[ "${pipeline_flag}" == "true" ]]; then
    di_check_vflow_pod ${EXTRA_TENANT_NAME} ${EXTRA_TENANT_USER_NAME}
  fi
}

unique_tenant_parameters(){
#DM01-2919 for rel-2209 and rel-2210, more parameters should be set in new tenant
#recently from rel-2215, all DI on_cloud need these parameters
  vflow_registry=$(vctl parameter get vflow.registry -t default)
  vflow_baseRegistry=$(vctl parameter get vflow.baseRegistry -t default)
  vflow_registrySecret=$(vctl parameter get vflow.registrySecret -t default)
  if [[ -n "${vflow_registry}" ]] && [[ -n "${vflow_baseRegistry}" ]] && [[ -n "${vflow_registrySecret}" ]]; then
    vctl parameter set vflow.registry ${vflow_registry} -t ${EXTRA_TENANT_NAME}
    vctl parameter set vflow.baseRegistry ${vflow_baseRegistry} -t ${EXTRA_TENANT_NAME}
    vctl parameter set vflow.registrySecret ${vflow_registrySecret} -t ${EXTRA_TENANT_NAME}
  else
    echo "DM01-2919: For rel-2209 and rel-2210, more parameters should be set in new tenant"
    exit 1 
  fi
}

# Create a unique tenant for each milestone validation job
create_unique_tenant(){
  RETRY=3
  vctl login ${VSYSTEM_ENDPOINT} ${VORA_SYSTEM_TENANT} ${VORA_USERNAME} -p "${VORA_SYSTEM_TENANT_PASSWORD}" --insecure
  for ((i = 1; i <= ${RETRY}; i++)); do
    vctl tenant create ${EXTRA_TENANT_NAME}
    if [[ $? == 0 ]]; then
      echo "success creat tenant ${EXTRA_TENANT_NAME}"
      vctl tenant set-strategy ${EXTRA_TENANT_NAME} sdi-default-extension-strategy
      # create vflow-secret for dhaas unique tetant, or the pipeline-modeler can not start.
      if [[ "${PROVISION_PLATFORM}" == "DHAAS-AWS" ]]; then
        kubectl get secret vflow-secret-rotated -o jsonpath='{.data.secret}' -n $NAMESPACE |base64 -d > secret.txt
        vctl secret create vflow-secret -f secret.txt -t ${EXTRA_TENANT_NAME}
        unique_tenant_parameters
      fi
      if [[ "${PROVISION_PLATFORM}" == "AZURE-AKS" ]]; then
        kubectl get secret vflow-secret -o jsonpath='{.data.secret}' -n $NAMESPACE |base64 -d > secret.txt
        vctl secret create vflow-secret -f secret.txt -t ${EXTRA_TENANT_NAME}
      fi
      break
    elif [ $i -eq ${RETRY} ]; then
      echo "### [level=error] The attempt to create a unique tenant has failed."
      exit 1
    else
      sleep 2
    fi
  done
}

# Delete a unique tenant for each milestone validation job
delete_unique_tenant(){
  vctl login ${VSYSTEM_ENDPOINT} ${VORA_SYSTEM_TENANT} ${VORA_USERNAME} -p "${VORA_SYSTEM_TENANT_PASSWORD}" --insecure
  vctl tenant delete ${EXTRA_TENANT_NAME}
  if [[ $? == 0 ]]; then
    echo "success delete tenant ${EXTRA_TENANT_NAME}"
  fi
}

di_check_vflow_pod(){
    pod_tenant_name=$1
    pod_tenant_user=$2

    set +e
    # check the pipeline-modeler status, the output should like 'pipeline-modeler-5328c71770c0ceee9a35a8-82g9z-6fbbfbfc49-gxx6c Running'
    pipeline_status=($(kubectl get pods -l vora-component=vflow,vsystem.datahub.sap.com/user=${pod_tenant_user},vsystem.datahub.sap.com/tenant=${pod_tenant_name} -n $NAMESPACE | awk 'NR==2{print $1, $3}'))
    set -e
    if [ -z "${pipeline_status}" ]; then
        echo "### [level=error] tenant ${pod_tenant_name} health_check: VFLOW INSTANCE check failed!"
        exit 1
    fi
    if [[ "${pipeline_status[1]}" != "Running" ]]; then
        echo "tenant ${pod_tenant_name} pipeline-modeler status: "${pipeline_status[@]}
        echo "### [level=error] tenant ${pod_tenant_name} health_check: pipeline status check failed!"
        exit 1
    fi
}

di_create_extra_tenant(){

    export EXTRA_TENANT_NAME=${EXTRA_TENANT_NAME}
    export EXTRA_TENANT_USER_NAME=${EXTRA_TENANT_USER_NAME}
    export EXTRA_TENANT_USER_PASSWORD=${EXTRA_TENANT_USER_PASSWORD}
    echo "export EXTRA_TENANT_NAME=${EXTRA_TENANT_NAME}" >> ${ENV_FILE}
    echo "export EXTRA_TENANT_USER_NAME=${EXTRA_TENANT_USER_NAME}" >> ${ENV_FILE}
    echo "export EXTRA_TENANT_USER_PASSWORD=${EXTRA_TENANT_USER_PASSWORD}" >> ${ENV_FILE}

    #get TmplateIds of default tenant
    vctl login ${VSYSTEM_ENDPOINT} ${VORA_TENANT} ${VORA_USERNAME} -p ${VORA_PASSWORD} --insecure
    TEMPLATEIDS=$(vctl scheduler list-instances |tail -n +2 |awk '{print $2}')
    echo $TEMPLATEIDS

    create_unique_tenant
    if [[ $? == 0 ]]; then
        # active the new tenant
        echo "## Creating a new user for extra tenant and starting all instances"
        user_role="tenantAdmin"
        create_unique_user ${EXTRA_TENANT_NAME} ${EXTRA_TENANT_USER_NAME} ${EXTRA_TENANT_USER_PASSWORD} ${user_role} && sleep 10 && start_scheduler

        echo "## Creating DI_DATA_LAKE connection..."
        export AUTH_HEADER='Authorization: Bearer '${IM_TOKEN}
        python /project/create_connection.py "${VSYSTEM_ENDPOINT}" ${PROVISION_PLATFORM} "${AUTH_HEADER}" ${EXTRA_TENANT_NAME} ${EXTRA_TENANT_USER_NAME} ${EXTRA_TENANT_USER_PASSWORD} "SDL"
        DI_DATA_LAKE_CONNECTION_CREATION_RESULT=$?
        if [[ $DI_DATA_LAKE_CONNECTION_CREATION_RESULT != 0 ]]; then
            echo "### [level=error] DI DATALAKE CONNECTION creation in the new tenant failed!"
            exit 1
        fi

    else
        die "## Create di tenant failed !!"
    fi
}

# Start a vFlow instance for the current user
start_vflow() {
  RETRY=6
  for ((i = 1; i <= ${RETRY}; i++)); do
    if vctl scheduler start pipeline-modeler; then
      break
    elif [ $i -eq ${RETRY} ]; then
      cat << EOF >> /infrabox/upload/archive/error_msg.log
health_check:No VFLOW pod running!
EOF
      die "### [level=error] Could NOT start pipeline modeler for user ${VORA_USERNAME}"
    else
      sleep 10
    fi
  done

  # check vflow pod status
  di_check_vflow_pod ${VORA_TENANT} ${VORA_USERNAME}
}

is_app_base(){
  if [[ "${CURRENT_COMPONENT}" == "datahub-app-base" ]] && [[ "${COMPONENT}" == "APP_BASE" ]]; then
    return 0
  else
    return 1
  fi
}

is_app_data(){
  if [[ "${CURRENT_COMPONENT}" == "datahub-app-data" ]] && [[ "${COMPONENT}" == "APP_DATA" ]]; then
    return 0
  else
    return 1
  fi
}

is_component_current_component(){
  if [[ -n "${CURRENT_COMPONENT}" ]] && (is_app_base || is_app_data); then
    return 0
    # add more checking for other components
  else
    return 1
  fi
}

checkout_code(){
  if is_component_current_component; then
    if [[ -n "${GITHUB_PULL_REQUEST_NUMBER}" ]]; then
      GIT_BRANCH="local_pull_request${INFRABOX_BUILD_NUMBER}"
      git fetch origin pull/${GITHUB_PULL_REQUEST_NUMBER}/head:${GIT_BRANCH}
    elif [[ -n "${INFRABOX_GIT_BRANCH}" ]]; then
      GIT_BRANCH=${INFRABOX_GIT_BRANCH}
    fi
  else
    set_branch_with_tag
  fi
  git checkout -qf $GIT_BRANCH
}

check_connection(){
  type=$1
  connection=$2
  TEST_DATA=$3
  RETRY_INSTANCE=5
  STATUS_CHECK_RESULT=1
  api="app/datahub-app-connection"
  echo "Try the api address ${api}"
  for((i=1;i<=${RETRY_INSTANCE};i++));
  do
    curl -vkL -X POST -H "X-Requested-With:Fetch" -H "Content-Type:application/json" -u "$VORA_TENANT\\${VORA_USERNAME}:${VORA_PASSWORD}" -d  "${TEST_DATA}" ${VSYSTEM_ENDPOINT}/${api}/connections
    sleep 30 # sleep for the connection ready
    set +e
    CONNECTION_STATUS=$(curl -k --user "$VORA_TENANT\\${VORA_USERNAME}:${VORA_PASSWORD}" -H "X-Requested-With" ${VSYSTEM_ENDPOINT}/${api}/connections/${connection}/status)
    echo ${CONNECTION_STATUS} | grep -i "status" | grep -i "OK"
    if [[ $? == 0 ]]; then
        echo "${type} Connection Status OK!"
        STATUS_CHECK_RESULT=0
        break
    else
        echo "WARNING! Connection status check with ${api}/connections/${connection}/status failed. retry ..."
    fi
    set -e
  done
  if [[ ${STATUS_CHECK_RESULT} != 0 ]];then
      echo "${type} Connection status check failed"
      exit 1
  fi
}

write_pull_result_to_xml() {
  result=$1
  failed_num="0"
  failed_msg=''
  if [ $result -ne 0 ]; then
    failed_num="1"
    failed_msg=`echo -e "\n      <failure message=\"docker pull from $2 failed!\" type=\"failure\"/>"`
  fi
  cat << EOF > /infrabox/upload/testresult/docker_registry_check.xml
<?xml version="1.0" ?>
<testsuites disabled="0" errors="0" failures="${failed_num}" tests="1" time="0.0">
  <testsuite disabled="0" errors="0" failures="${failed_num}" name="DOCKER_REGISTRY_check" skipped="0" tests="1" time="0">
    <testcase classname="docker_registry_check" name="DOCKER_REGISTRY">${failed_msg}
    </testcase>
  </testsuite>
</testsuites>
EOF
}

write_vflow_pod_check_to_xml() {
  result=$1
  failed_num="0"
  failed_msg=''
  if [ $result -ne 0 ]; then
    failed_num="1"
    failed_msg=`echo -e "\n      <failure message=\"VFLOW INSTANCE check failed!\" type=\"failure\"/>"`
  fi

  cat << EOF > /infrabox/upload/testresult/vflow_instance_check.xml
<?xml version="1.0" ?>
<testsuites disabled="0" errors="0" failures="${failed_num}" tests="1" time="0.0">
  <testsuite disabled="0" errors="0" failures="${failed_num}" name="VFLOW_INSTANCE_CHECK" skipped="0" tests="1" time="0">
    <testcase classname="vflow_instance_check" name="VFLOW_INSTANCE_CHECK">${failed_msg}
    </testcase>
  </testsuite>
</testsuites>
EOF
}

docker_pull () {
  set +e
  service docker status
  RETRY_PULL=6
  for ((loop = 1; loop <= ${RETRY_PULL}; loop++)); do
    docker pull $1
    if [ $? -ne 0 ]; then
      if [[ ${loop} < ${RETRY_PULL} ]]; then
        echo "docker pull failed. retry..."
        service docker start
        sleep 20
        service docker status
        continue
      fi
      echo "## Warning! Docker pull failed even retry 6 times! Exit the job!"
      exit 1
    else
      echo "docker pull sucessfully."
      break
    fi
  done
  set -e
}
# Gett the components version from env.sh
if [ -n "${PARENT_INSTALL_JOB}" ]; then
  ENV_FILE="/infrabox/inputs/${PARENT_INSTALL_JOB}/env.sh"
  if [ ! -f ${ENV_FILE} ]; then
    echo "${ENV_FILE} does not exist."
    exit 1
  else
    source ${ENV_FILE}
  fi
else
  echo "### [level=warning] Skip sourcing env.sh, Using external env information..."
fi

source /project/common.sh

if [[ -n ${PARENT_INSTALL_JOB} ]] && [[ -f "/infrabox/inputs/${PARENT_INSTALL_JOB}/dev_config.sh" ]]; then
  source /infrabox/inputs/${PARENT_INSTALL_JOB}/dev_config.sh
  DOCKER_CUSTOM_INSEC_OPTION=$(python /project/update_docker_daemon.py /infrabox/inputs/${PARENT_INSTALL_JOB}/dev_config.sh true)
fi

LOG_OUTPUT_DIR="/infrabox/output/execution_log/build_and_run_images"
mkdir -p $LOG_OUTPUT_DIR
REGISTRY="di-dev-cicd-docker.int.repositories.cloud.sap/bdh-infra-tools"
BUILD_ARG=""

# cut the -SNAPSHOT suffix eg: 2.6.51-SNAPSHOT -> 2.6.51
if [[ ${VSYSTEM_VERSION} == *-SNAPSHOT* ]]
then
    VSYSTEM_SNAPSHOT=${VSYSTEM_VERSION}
    echo "Over write $VSYSTEM_VERSION by ${VSYSTEM_VERSION%-SNAPSHOT*}"
    export VSYSTEM_VERSION=${VSYSTEM_VERSION%-SNAPSHOT*}
    sed -i "/VSYSTEM_VERSION/s/${VSYSTEM_SNAPSHOT}/${VSYSTEM_VERSION}/g" ${ENV_FILE}
fi

# set vctl bin path
if [ ! -f /infrabox/inputs/${PARENT_INSTALL_JOB}/vctl ]; then
  # DHAAS or other situation
  set +e
  download_vctl
  cp ./vctl /infrabox/inputs/${PARENT_INSTALL_JOB}/vctl
  set -e
fi
export VCTL_BIN="/infrabox/inputs/${PARENT_INSTALL_JOB}/vctl"
cp ${VCTL_BIN} /usr/local/bin/

# create dis tenant
if [[ -n ${DIS_VERSION} ]]; then
  echo "Starting to create DI:E tenant, and do some test."
  tenant_id=$(cat /proc/sys/kernel/random/uuid)

  cat /infrabox/inputs/${PARENT_INSTALL_JOB}/shoot-kubeconfig.yaml
  python /dis_create_tenant.py /infrabox/inputs/${PARENT_INSTALL_JOB}/shoot-kubeconfig.yaml ${tenant_id} ${ENV_FILE}
  if [ $? -ne 0 ]; then
    echo "create DI:E tenant failed. and exit"
    exit 1
  fi
fi

echo "COMPONENT=$COMPONENT"
echo "GIT_REPO=$GIT_REPO"
echo "IMAGE_TAG=$IMAGE_TAG"

if [ -n "${IMAGE_TAG}" ]; then
    IMAGE_TAG=":${IMAGE_TAG}"
else
    IMAGE_TAG=":$INFRABOX_BUILD_NUMBER"
fi

echo "DOCKER_OPTS=\" ${DOCKER_CUSTOM_INSEC_OPTION} --insecure-registry=di-dev-cicd-docker.int.repositories.cloud.sap --insecure-registry=public.int.repositories.cloud.sap \
--insecure-registry=di-dev-cicd-v2.int.repositories.cloud.sap \
--insecure-registry=v2-registry.dhcp.wdf.sap.corp \
--insecure-registry=docker.wdf.sap.corp:51088 \
--insecure-registry=docker.wdf.sap.corp:51022 \
--insecure-registry=docker.wdf.sap.corp:51055 \
--insecure-registry=docker.wdf.sap.corp:65170 \
--insecure-registry=docker.wdf.sap.corp:51021 \
--insecure-registry=docker.wdf.sap.corp:50002 \
--insecure-registry=docker.wdf.sap.corp:50000 \
--insecure-registry=xmake-deploy-snapshot.int.repositories.cloud.sap \
--insecure-registry=10.47.221.71:5000 \
--insecure-registry=velocity-0.dhcp.ykf.sap.corp:10000 \
--insecure-registry=di-dev-cicd-docker.int.repositories.cloud.sap --insecure-registry=registry-backup.datahub.only.sap\"" >> /etc/default/docker
service docker restart
sleep 10
# wait for docker service running
set +e
RETRY=6
for ((loop = 1; loop <= ${RETRY}; loop++)); do
  service docker status | grep "Docker is running"
  if [ $? -ne 0 ]; then
     if [[ ${loop} < ${RETRY} ]]; then
       echo "docker deamon is not ready. retry..."
       service docker start
       sleep 10
       continue
     fi
     echo "## Warning! Docker daemon is not running! Exit the job!"
     exit 1
  else
     echo "docker daemon is running."
     break
  fi
done
git config --global http.sslVerify false
service docker status
set -e
if [ -n "${GIT_REPO}" ]; then
    git clone ${GIT_REPO}
    REPO_FOLDER=$(echo ${GIT_REPO##*/} | awk -F  . '{print $1}')
    pushd $REPO_FOLDER
      checkout_code
      git log -20 --date-order > /infrabox/upload/archive/recent_commit.log
      cp /infrabox/upload/archive/recent_commit.log ${LOG_OUTPUT_DIR}
      if [ $COM_PATCHSET_REF ];then
        git fetch ${GIT_REPO} $COM_PATCHSET_REF && git checkout FETCH_HEAD
      fi
    popd
else
    echo "WARNING! No repository set up"
fi

if [[ ${COM_BUILD_ARG} ]] && [[ -n ${COM_BUILD_ARG} ]]; then
    if [[ ${COM_BUILD_ARG} == "INFRABOX_BUILD_NUMBER" ]]; then
        BUILD_ARG="--build-arg INFRABOX_BUILD_NUMBER=${INFRABOX_BUILD_NUMBER}"
    else
        BUILD_ARG=${COM_BUILD_ARG}
    fi
fi

# docker pull and rename the tag with INFRABOX_BUILD_NUMBER
if [[ -n $COM_DOCKER_IMAGE ]]; then
  if [[ -n ${TEST_IMAGE_DEV_CONFIG_SUFFIX} ]]; then
    TEST_IMAGE_DEV_CONFIG_SOURCE_CUSTOM=`eval echo '$'"${TEST_IMAGE_DEV_CONFIG_SUFFIX}_SOURCE_CUSTOM"`
    TEST_IMAGE_DEV_CONFIG_TAG_CUSTOM=`eval echo '$'"${TEST_IMAGE_DEV_CONFIG_SUFFIX}_TAG_CUSTOM"`
    if [[ -n ${TEST_IMAGE_DEV_CONFIG_SOURCE_CUSTOM} ]] && [[ -n ${TEST_IMAGE_DEV_CONFIG_TAG_CUSTOM} ]]; then
      TEST_COMPONENT_VERSION=${TEST_IMAGE_DEV_CONFIG_TAG_CUSTOM}
      # replace image path with ${TEST_IMAGE_DEV_CONFIG_SOURCE_CUSTOM}
      COM_DOCKER_IMAGE=${TEST_IMAGE_DEV_CONFIG_SOURCE_CUSTOM}/${COM_DOCKER_IMAGE#*/}
    fi
  fi
  if [[ -n ${CUSTOME_IMAGE_TAG} ]]; then
    TEST_COMPONENT_VERSION=${CUSTOME_IMAGE_TAG}
  fi
  if [[ -z "${TEST_COMPONENT_VERSION}" ]] && [[ -n ${COMPONENT} ]]; then
    # some test image is built with the hanalite-releasepack. So the version of test image is the same with the one of hanalite-releasepack. like the test docker image of RMS and E2E
    if [[ $COMPONENT == 'RELEASEPACK' ]] && [[ -n ${DIS_VERSION} ]]; then
        TEST_COMPONENT_VERSION=${DIS_VERSION%%-*}
    else
        TEST_COMPONENT_VERSION=`eval echo '$'"$COMPONENT""_VERSION"`
    fi
  fi
  if [[ -n "${TEST_COMPONENT_VERSION}" ]]; then
      docker_pull $COM_DOCKER_IMAGE:$TEST_COMPONENT_VERSION
      write_pull_result_to_xml $? $COM_DOCKER_IMAGE
      docker tag $COM_DOCKER_IMAGE:$TEST_COMPONENT_VERSION $REGISTRY/$COMPONENT_TEST_NAME${IMAGE_TAG}
  else
      echo "NO COMPONENT verstion set."
  fi
elif [[ ${DOCKER_WORK_DIR} ]] && [[ -n ${DOCKER_WORK_DIR} ]]; then
    pushd ${DOCKER_WORK_DIR}
    docker build -f /$REPO_FOLDER/$TEST_FOLDER/$DOCKERFILE -t $REGISTRY/${COMPONENT_TEST_NAME}${IMAGE_TAG} . $BUILD_ARG
elif [[ -n $COSTOMIZED_DOCKER_IMAGE ]]; then
    TEST_COMPONENT_VERSION=build_${INFRABOX_BUILD_NUMBER}
    docker_pull $COSTOMIZED_DOCKER_IMAGE:$TEST_COMPONENT_VERSION
    write_pull_result_to_xml $? $COSTOMIZED_DOCKER_IMAGE
    docker tag $COSTOMIZED_DOCKER_IMAGE:$TEST_COMPONENT_VERSION $REGISTRY/$COMPONENT_TEST_NAME${IMAGE_TAG}
else
    pushd $REPO_FOLDER/$TEST_FOLDER
    docker build -f $DOCKERFILE -t $REGISTRY/$COMPONENT_TEST_NAME${IMAGE_TAG} . $BUILD_ARG
fi

set +x
#add env variable IM_TOKEN to ENV file
if [[ -n "${IM_TOKEN}" ]];then
    export IM_TOKEN=${IM_TOKEN}
else
    auth_header_file=$(find /infrabox/inputs -mindepth 2 -maxdepth 2 -name auth_header.txt)
    if ! [ -f "$auth_header_file" ]; then
        echo "### [level=error] No IM_TOKEN found in ENV, test would be fail."
    else
        IM_TOKEN=$(cat $auth_header_file)
        export IM_TOKEN=${IM_TOKEN/Authorization: Bearer /}
    fi
fi
set -x

# flag for DM01-2802: default value is True, 
# from vsolution 2210.6.0, no need to start pipeline-modeler, set this value to False.
pipeline_flag=true
update_pipeline_flag

if [[ -n "$EXTRA_TENANT" ]]; then
    echo "## Creating an extra tenant"
    tenant_name_str="tenant-$(cat /proc/sys/kernel/random/uuid |cut -c 1-18)"
    EXTRA_TENANT_NAME="${EXTRA_TENANT_NAME:-$tenant_name_str}"
    # use the same user/password as 'default tenant', if extra tenant user/password is not set.
    EXTRA_TENANT_USER_NAME="${EXTRA_TENANT_USER_NAME:-${VORA_USERNAME}}"
    EXTRA_TENANT_USER_PASSWORD="${EXTRA_TENANT_USER_PASSWORD:-${VORA_PASSWORD}}"

    di_create_extra_tenant
else
    # for vflow graph tests
    # Login using default user name first and then create a new user
    if [[ "${VFLOW_GRAPH_TEST}" == "TRUE" ]]; then
        echo "### Start create a new user for vflow graph tests"
        vctl login ${VSYSTEM_ENDPOINT} ${VORA_TENANT} ${VORA_USERNAME} -p ${VORA_PASSWORD} --insecure
        VORA_USERNAME="vflow-$(cat /proc/sys/kernel/random/uuid)"
        vflow_user_role="member"
        create_unique_user ${VORA_TENANT} ${VORA_USERNAME} ${VORA_PASSWORD} ${vflow_user_role}
        # DM01-2802 from vsolution 2210.6.0, no need to start pipeline-modeler
        if [[ "${pipeline_flag}" == "true" ]]; then
            sleep 10 && start_vflow
        fi
        export VORA_USERNAME=${VORA_USERNAME}
    fi
fi

# Add the ABAP connection set-up
if [[ -n "${ABAP_CONNECTION_ID}" ]];then
    type='ABAP'
    connection=${ABAP_CONNECTION_ID}
    TEST_DATA="{ \"id\": \"${ABAP_CONNECTION_ID}\", \"type\": \"ABAP\", \"description\": \"ABAP connection\",\"licenseRelevant\": false,\"tags\": [],\"contentData\": {\"configurationType\": \"Manual\", \"connectionID\": \"${ABAP_CONNECTION_ID}\", \"protocol\": \"${PROTOCOL}\",\"sysid\": \"${SYSID}\",\"client\": \"${CLIENT}\",\"user\": \"${ABAP_USER}\",\"password\": \"${ABAP_PASS}\",\"sysnr\": \"${SYSNR}\", \"ashost\": \"${ASHOST}\",\"gwhost\": \"${GWHOST}\",\"sap_language\": \"EN\" } }"
    check_connection ${type} ${connection} ${TEST_DATA}
fi

# add extra envs for vsystem test
set_env_for_vsystem_test

#source env agin for some I:E tenant env are added into env.sh
if [[ -n ${DIS_VERSION} ]]; then
  set +x
  source ${ENV_FILE} > /dev/null
  set -x
fi

# extend the container env.
RUNTIME_ENV_FILE="runtime.env"
# workaround. TODO use python to do this
unset GERRIT_CHANGE_COMMIT_MESSAGE
unset DEV_CONFIG_FILE_CONTENT
env | grep -vw 'no_proxy\|HOSTNAME\|HOME\|https_proxy\|http_proxy\|TERM\|PATH\|PWD\|LS_COLORS\|SHLVL\|LESSOPEN\|LESSCLOSE' | awk  -F '=' '{print $1}' > ${RUNTIME_ENV_FILE}
RUNTIME_ENV_FILE_OPTION="--env-file ${RUNTIME_ENV_FILE}"
#cat $RUNTIME_ENV_FILE

set +e
docker run -v /infrabox:/infrabox ${DOCKER_SOCK_OPTION} --privileged -e "ENV_FILE=${ENV_FILE}" ${RUNTIME_ENV_FILE_OPTION} ${REGISTRY}/${COMPONENT_TEST_NAME}${IMAGE_TAG}
job_status=$?
if [[ "${ENABLE_DEPLOYMENTS}" == "yes" ]] && [[ ${JOB_NAME} ]]; then
  docker tag ${REGISTRY}/${COMPONENT_TEST_NAME}${IMAGE_TAG} ${REGISTRY}/troubleshooting/${JOB_NAME}:build_${INFRABOX_BUILD_NUMBER}
  docker push ${REGISTRY}/troubleshooting/${JOB_NAME}:build_${INFRABOX_BUILD_NUMBER}
fi

#upload result to cumulus
if [[ -n ${DIS_VERSION} ]]; then
  upload_result_cumulus
fi

set +e
if [[ "${ENABLE_K8S_DUMP}" == "yes" ]]; then
  dump_k8s_cluster_info
fi

if [[ -n $EXTRA_TENANT ]]; then
  echo "Starting to delete tenant"
  delete_unique_tenant
fi

# delete DI:E tenant
if [[ -n ${DIS_VERSION} ]]; then
  echo "Starting to delete DI:E tenant, after test done."
  python /dis_delete_tenant.py /infrabox/inputs/${PARENT_INSTALL_JOB}/shoot-kubeconfig.yaml ${tenant_id}
fi

if [[ ${job_status} -ne 0 ]];then
  echo "run ${COMPONENT_TEST_NAME} failed! status code is ${job_status}"
  exit 1
fi
echo "Run ${COMPONENT_TEST_NAME} complete!"



